{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Machine Learning Final Project**\nBelow is the code that we use to submit into the \"Optiver: Trading at The Close\" competition. For total, we have tried 3 model, which are (with their best score):\n\n**1. XGBoost (5.5072)** [](http://)https://www.kaggle.com/code/re6125015ncku/final-test/notebook\n\n**2. Catboost (5.3443)** [](http://)https://www.kaggle.com/re6125015ncku/catboost\n\n**3. LightGBM (5.3360)** [](http://)https://www.kaggle.com/code/re6125015ncku/lightgbm/notebook\n\nAll of these are done with feature engineering (description below), and cross validation to determine the best parameter for each model, and to validate the results.\n\nThe code below are a combine code with three models added, and at the end, the one we chose is the LightGBM model which return the lowest MAE.","metadata":{}},{"cell_type":"markdown","source":" # **Feature Engineering**\n This part is referenced from:\n 1. The remarkable work of Angle, [](http://)https://www.kaggle.com/code/lblhandsome/optiver-robust-best-single-model/notebook\n 2. The explanation from Zulqarnain Ali, [](http://)https://www.kaggle.com/code/zulqarnainali/explained-singel-model-optiver","metadata":{}},{"cell_type":"code","source":"# Importing necessary libraries\nimport os\nimport gc\nimport time\nimport warnings\nfrom warnings import simplefilter\nfrom itertools import combinations\n\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom numba import njit, prange\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold, GridSearchCV, TimeSeriesSplit, cross_val_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.impute import SimpleImputer\n\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor, EShapCalcType, EFeaturesSelectionAlgorithm\nimport lightgbm as lgb  # LightGBM gradient boosting framework\n\nwarnings.filterwarnings(\"ignore\")\nsimplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up parameters\nis_offline = False    # Flag for online/offline mode\nis_train = True    # Flag for training mode\nis_infer = True    # Flag for inference mode\nsplit_day = 435    # Split day for time series data\nmax_lookback = np.nan\n\ndf = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\ndf = df.dropna(subset=[\"target\"])\ndf.reset_index(drop=True, inplace=True)\ndf.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weighted_average(a):\n    w = []\n    n = len(a)\n    for j in range(1, n + 1):\n        j = 2 if j == 1 else j\n        w.append(1 / (2**(n + 1 - j)))\n    return w","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=0):\n    \"\"\"\n    Iterate through all numeric columns of a dataframe and modify the data type\n    to reduce memory usage.\n    \"\"\"\n    # Calculate the initial memory usage of the DataFrame\n    start_mem = df.memory_usage().sum() / 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            \n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                # Check if the column's data type is a float\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float32)\n                    \n    if verbose:\n        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n        end_mem = df.memory_usage().sum() / 1024**2\n        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n        decrease = 100 * (start_mem - end_mem) / start_mem\n        logger.info(f\"Decreased by {decrease:.2f}%\")\n\n    return df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to compute triplet imbalance in parallel using Numba\n@njit(parallel=True)\ndef compute_triplet_imbalance(df_values, comb_indices):\n    num_rows = df_values.shape[0]\n    num_combinations = len(comb_indices)\n    imbalance_features = np.empty((num_rows, num_combinations))\n\n    # Loop through all combinations of triplets\n    for i in prange(num_combinations):\n        a, b, c = comb_indices[i]\n        \n        # Loop through rows of the DataFrame\n        for j in range(num_rows):\n            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n            \n            # Prevent division by zero\n            if mid_val == min_val:\n                imbalance_features[j, i] = np.nan\n            else:\n                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n\n    return imbalance_features\n\n\n# Function to calculate triplet imbalance for given price data and a DataFrame\ndef calculate_triplet_imbalance_numba(price, df):\n    # Convert DataFrame to numpy array for Numba compatibility\n    df_values = df[price].values\n    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n\n    # Calculate the triplet imbalance using the Numba-optimized function\n    features_array = compute_triplet_imbalance(df_values, comb_indices)\n\n    # Create a DataFrame from the results\n    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n    features = pd.DataFrame(features_array, columns=columns)\n\n    return features","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def imbalance_features(df):\n    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n    \n    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n\n    for c in combinations(prices, 2):\n        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n\n    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n        df[triplet_feature.columns] = triplet_feature.values\n    \n    df[\"stock_weights\"] = df[\"stock_id\"].map(weights)\n    df[\"weighted_wap\"] = df[\"stock_weights\"] * df[\"wap\"]\n    df['wap_momentum'] = df.groupby('stock_id')['weighted_wap'].pct_change(periods=6)\n   \n    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n    \n    df['spread_depth_ratio'] = (df['ask_price'] - df['bid_price']) / (df['bid_size'] + df['ask_size'])\n    df['mid_price_movement'] = df['mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n    \n    df['micro_price'] = ((df['bid_price'] * df['ask_size']) + (df['ask_price'] * df['bid_size'])) / (df['bid_size'] + df['ask_size'])\n    df['relative_spread'] = (df['ask_price'] - df['bid_price']) / df['wap']\n    \n    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n        \n\n    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n        for window in [1,3,5,10]:\n            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n    \n    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'weighted_wap','price_spread']:\n        for window in [1,3,5,10]:\n            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n    \n    for window in [3,5,10]:\n        df[f'price_change_diff_{window}'] = df[f'bid_price_diff_{window}'] - df[f'ask_price_diff_{window}']\n        df[f'size_change_diff_{window}'] = df[f'bid_size_diff_{window}'] - df[f'ask_size_diff_{window}']\n\n    pl_df = pl.from_pandas(df)\n\n    windows = [3, 5, 10]\n    columns = ['ask_price', 'bid_price', 'ask_size', 'bid_size']\n\n    group = [\"stock_id\"]\n    expressions = []\n\n    for window in windows:\n        for col in columns:\n            rolling_mean_expr = (\n                pl.col(f\"{col}_diff_{window}\")\n                .rolling_mean(window)\n                .over(group)\n                .alias(f'rolling_diff_{col}_{window}')\n            )\n\n            rolling_std_expr = (\n                pl.col(f\"{col}_diff_{window}\")\n                .rolling_std(window)\n                .over(group)\n                .alias(f'rolling_std_diff_{col}_{window}')\n            )\n\n            expressions.append(rolling_mean_expr)\n            expressions.append(rolling_std_expr)\n\n    lazy_df = pl_df.lazy().with_columns(expressions)\n\n    pl_df = lazy_df.collect()\n\n    df = pl_df.to_pandas()\n    gc.collect()\n    \n    df['mid_price*volume'] = df['mid_price_movement'] * df['volume']\n    df['harmonic_imbalance'] = df.eval('2 / ((1 / bid_size) + (1 / ask_size))')\n    \n    for col in df.columns:\n        df[col] = df[col].replace([np.inf, -np.inf], 0)\n\n    return df\n\ndef other_features(df):\n    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  \n    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  \n    df['time_to_market_close'] = 540 - df['seconds_in_bucket']\n    \n    for key, value in global_stock_id_feats.items():\n        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n\n    return df\n\ndef generate_all_features(df):\n    cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\", \"target\"]]\n    df = df[cols]\n    \n    df = imbalance_features(df)\n    gc.collect() \n    df = other_features(df)\n    gc.collect()  \n    feature_name = [i for i in df.columns if i not in [\"row_id\", \"target\", \"time_id\", \"date_id\"]]\n    \n    return df[feature_name]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights = [\n    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n]\nweights = {int(k):v for k,v in enumerate(weights)}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if the code is running in offline or online mode\nif is_offline:\n    # In offline mode, split the data into training and validation sets based on the split_day\n    df_train = df[df[\"date_id\"] <= split_day]\n    df_valid = df[df[\"date_id\"] > split_day]\n    print(\"Offline mode\")\n    print(f\"train : {df_train.shape}, valid : {df_valid.shape}\")\nelse:\n    # In online mode, use the entire dataset for training\n    df_train = df\n    print(\"Online mode\")\n    \ndel df\ngc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if is_train:\n    global_stock_id_feats = {\n        \"median_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].median() + df_train.groupby(\"stock_id\")[\"ask_size\"].median(),\n        \"std_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].std() + df_train.groupby(\"stock_id\")[\"ask_size\"].std(),\n        \"ptp_size\": df_train.groupby(\"stock_id\")[\"bid_size\"].max() - df_train.groupby(\"stock_id\")[\"bid_size\"].min(),\n        \"median_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].median() + df_train.groupby(\"stock_id\")[\"ask_price\"].median(),\n        \"std_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].std() + df_train.groupby(\"stock_id\")[\"ask_price\"].std(),\n        \"ptp_price\": df_train.groupby(\"stock_id\")[\"bid_price\"].max() - df_train.groupby(\"stock_id\")[\"ask_price\"].min(),\n    }\n    if is_offline:\n        df_train_feats = generate_all_features(df_train)\n        print(\"Build Train Feats Finished.\")\n        df_valid_feats = generate_all_features(df_valid)\n        print(\"Build Valid Feats Finished.\")\n        df_valid_feats = reduce_mem_usage(df_valid_feats)\n    else:\n        df_train_feats = generate_all_features(df_train)\n        print(\"Build Online Train Feats Finished.\")\n\n    df_train_feats = reduce_mem_usage(df_train_feats)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **XGBoost**\n\nFor results and visualization, please go to https://www.kaggle.com/code/re6125015ncku/final-test/notebook","metadata":{}},{"cell_type":"code","source":"# Train procedure\nif is_train:\n    offline_split = df_train['date_id']>(split_day - 45)\n    X_train = df_train_feats[~offline_split]\n    X_val = df_train_feats[offline_split]\n    y_train = df_train['target'][~offline_split]\n    y_val = df_train['target'][offline_split]\n    del df_train\n    gc.collect()\n    \nX_train.shape, X_val.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Hyperparameter tuning by using GridSearchCV and TimeSeriesSplit, as the dataset are time\n## series dataset\nif is_train:\n    cv_split = TimeSeriesSplit(n_splits=5)\n\n    param_grid = {\n        'learning_rate': [0.1],\n        'n_estimators': [200],\n        'subsample': [0.6, 0.7],\n        'colsample_bytree': [0.6, 0.7],\n        'reg_alpha':[0.5],\n        'gamma':[0.5],\n\n        ##fixed_dict\n        'booster': ['gbtree'],\n        'max_depth': [8],\n        'min_child_weight': [3],\n        'grow_policy': ['depthwise'],\n        'objective': ['reg:absoluteerror'],\n        'num_class': [1],\n        'device' : ['gpu'],\n        'eval_metric' : ['mae'],\n        'random_state' : [42],\n        'tree_method' : ['hist']\n    }\n\n    # Create the XGBoost model object\n    xgb_model = xgb.XGBRegressor()\n    # Create the GridSearchCV object\n    grid_search = GridSearchCV(xgb_model, param_grid, cv=cv_split)\n    # Fit the GridSearchCV object to the training data\n    grid_search.fit(X_train, y_train)\n    # Print the best set of hyperparameters and the corresponding score\n    print(\"Best set of hyperparameters: \", grid_search.best_params_)\n    print(\"Best score: \", grid_search.best_score_)\n\n    best_params = grid_search.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## We adjust the parameter based on the value return from the best_params\nclf = xgb.XGBRegressor(booster='gbtree', colsample_bytree=1.0, device='gpu', eval_metric='mae', gamma=0.5, grow_policy='depthwise', \n                       learning_rate=0.1, max_depth=4, min_child_weight=3, n_estimators=200, \n                       num_class=1, objective='reg:absoluteerror', random_state=42, reg_alpha=0.5, subsample=0.7, tree_method='hist')    ### version 4 -> 5.5131\n# clf = xgb.XGBRegressor(**best_params)\nclf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)], early_stopping_rounds=100, verbose=0)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = clf.predict(X_val)\nmae = mean_absolute_error(y_pred, y_val)\nprint(\"Mean Absolute Error: {}\".format(mae))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **CatBoost**\n\nFor results and visualization, please go to https://www.kaggle.com/re6125015ncku/catboost","metadata":{}},{"cell_type":"code","source":"## Hyperparameter tuning by using GridSearchCV and TimeSeriesSplit, as the dataset are time\n## series dataset\nif is_train:\n    train_data = df_train_feats\n    train_labels = df_train['target']\n    cv_split = TimeSeriesSplit(n_splits=5)\n\n    param_grid = {\n        'learning_rate': [1.0, 0.1, 0.01],\n        'depth': [4, 6, 8, 10],\n        'l2_leaf_reg': [10, 30, 50],\n        'iterations': [1200],\n        'bootstrap_type': ['Bernoulli'],\n        'subsample': [0.66],\n        'od_type': ['Iter'],\n        'od_wait': [30],\n        'allow_writing_files': [False],\n    }\n\n    ctb_model = CatBoostRegressor(loss_function='MAE', eval_metric='MAE', task_type='GPU')\n    # Create the GridSearchCV object\n    grid_search = GridSearchCV(ctb_model, param_grid, cv=cv_split, scoring='neg_mean_absolute_error', verbose=1)\n\n    # Fit the GridSearchCV object to the training data\n    grid_search.fit(train_data, train_labels, verbose=0)\n\n    # Print the best set of hyperparameters and the corresponding score\n    print(\"Best set of hyperparameters: \", grid_search.best_params_)\n    print(\"Best score: \", grid_search.best_score_)\n    best_params = grid_search.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train procedure\nif is_train:\n    offline_split = df_train['date_id']>(split_day - 45)\n    df_offline_train = df_train_feats[~offline_split]\n    df_offline_valid = df_train_feats[offline_split]\n    df_offline_train_target = df_train['target'][~offline_split]\n    df_offline_valid_target = df_train['target'][offline_split]\n    df_train_target = df_train[\"target\"]\n    del df_train\n    gc.collect()\n    \n    ## We adjust the parameter based on the value return from the best_params\n    ctb_params = dict(iterations=1200,\n                      learning_rate=1.0,\n                      depth=8,\n                      l2_leaf_reg=30,\n                      bootstrap_type='Bernoulli',\n                      subsample=0.66,\n                      loss_function='MAE',\n                      eval_metric = 'MAE',\n                      metric_period=100,\n                      od_type='Iter',\n                      od_wait=30,\n                      task_type='GPU',\n                      allow_writing_files=False,\n                      )\n    \n    print(\"Feature Elimination Performing.\")\n    ctb_model = CatBoostRegressor(**ctb_params)\n    summary = ctb_model.select_features(\n        df_offline_train[feature_name], df_offline_train_target,\n        eval_set=[(df_offline_valid[feature_name], df_offline_valid_target)],\n        features_for_select=feature_name,\n        num_features_to_select=len(feature_name)-24,    # Dropping from 124 to 100\n        steps=3,\n        algorithm=EFeaturesSelectionAlgorithm.RecursiveByShapValues,\n        shap_calc_type=EShapCalcType.Regular,\n        train_final_model=False,\n        plot=True,\n    )\n    \n    print(\"Valid Model Training on Selected Features Subset.\")\n    ctb_model = CatBoostRegressor(**ctb_params)\n    ctb_model.fit(\n        df_offline_train[summary['selected_features_names']], df_offline_train_target,\n        eval_set=[(df_offline_valid[summary['selected_features_names']], df_offline_valid_target)],\n        use_best_model=True,\n    )\n    \n    del df_offline_train, df_offline_valid, df_offline_train_target, df_offline_valid_target\n    gc.collect()\n    \n    print(\"Infer Model Training on Selected Features Subset.\")\n    infer_params = ctb_params.copy()\n    # CatBoost train best with Valid number of iterations\n    infer_params[\"iterations\"] = ctb_model.best_iteration_\n    infer_ctb_model = CatBoostRegressor(**infer_params)\n    infer_ctb_model.fit(df_train_feats[summary['selected_features_names']], df_train_target)\n    print(\"Infer Model Training on Selected Features Subset Complete.\")\n    \n    if is_offline:   \n        # Offline predictions\n        df_valid_target = df_valid[\"target\"]\n        offline_predictions = infer_ctb_model.predict(df_valid_feats[summary['selected_features_names']])\n        offline_score = mean_absolute_error(offline_predictions, df_valid_target)\n        print(f\"Offline Score {np.round(offline_score, 4)}\")\n        del df_valid, df_valid_feats\n        gc.collect()\n    \n    del df_train_feats\n    gc.collect()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Generates a bar plot visualizing the feature importances of our model, \n## making it easier to identify which features are more influential in the model's predictions.\nfeat_importances = infer_ctb_model.get_feature_importance(prettified=True)\n\nplt.figure(figsize=(12, 20))\nsns.barplot(x=\"Importances\", y=\"Feature Id\", data=feat_importances)\nplt.title('CatBoost features importance:')\nplt.tight_layout()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Check for top interactions in the model.\nfeat_interactions = infer_ctb_model.get_feature_importance(type=EFstrType.Interaction, prettified=True)\ntop_interactions = feat_interactions[:10]\ntop_interactions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Change above in index to column, making it easier to\n## identify which features interaction are more influential in the model's predictions.\ntop_interactions['First Feature Index'] = top_interactions['First Feature Index'].apply(lambda x: summary['selected_features_names'][x])\ntop_interactions['Second Feature Index'] = top_interactions['Second Feature Index'].apply(lambda x: summary['selected_features_names'][x])\ntop_interactions.columns = ['First Feature', 'Second Feature', 'Interaction']\ntop_interactions","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **LightGBM**\n\nFor results and visualization, please go to https://www.kaggle.com/code/re6125015ncku/lightgbm/notebook","metadata":{}},{"cell_type":"code","source":"## Hyperparameter tuning by using GridSearchCV and TimeSeriesSplit, as the dataset are time\n## series dataset\ntrain_data = df_train_feats\ntrain_labels = df_train['target']\ncv_split = TimeSeriesSplit(n_splits=5)\n\nparam_grid = {\n    'num_leaves': [128, 256, 512],\n    'subsample': [0.4, 0.6, 0.8, 1.0],\n    'colsample_bytree': [0.4, 0.6, 0.8, 1.0],\n    'learning_rate': [0.001, 0.01, 0.1, 1.0],\n    'maxdepth': [3, 11, 24],\n    'reg_alpha': [0.1, 0.2, 0.3],\n    'reg_lambda': [0.2]\n    'objective': ['mae'],\n    'n_estimators': [5000],\n    'device': ['gpu'],\n    'n_jobs': [-1],\n    'importance_type': [gain],\n}\n\nlgb_model = LGBMRegressor(loss_function='MAE', eval_metric='MAE', task_type='GPU')\n# Create the GridSearchCV object\ngrid_search = GridSearchCV(lgb_model, param_grid, cv=cv_split, scoring='neg_mean_absolute_error', verbose=1)\n\n# Fit the GridSearchCV object to the training data\ngrid_search.fit(train_data, train_labels, verbose=0)\n\n# Print the best set of hyperparameters and the corresponding score\nprint(\"Best set of hyperparameters: \", grid_search.best_params_)\nprint(\"Best score: \", grid_search.best_score_)\nbest_params = grid_search.best_params_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## We adjust the parameter based on the value return from the best_params\nlgb_params = {\n    \"objective\": \"mae\",\n    \"n_estimators\": 5000,\n    \"num_leaves\": 256,       # 512\n    \"subsample\": 0.6,        # 0.4\n    \"colsample_bytree\": 0.8, # 0.6\n    \"learning_rate\": 0.01,   # 0.001\n    'max_depth': 11,         # 24\n    \"n_jobs\": -1,\n    \"device\": \"gpu\",\n    \"verbosity\": -1,\n    \"importance_type\": \"gain\",\n    \"reg_alpha\": 0.2,        # 0.1\n    \"reg_lambda\": 3.25\n}\n\nfeature_columns = list(df_train_feats.columns)\nprint(f\"Features = {len(feature_columns)}\")\n    \nnum_folds = 5\nfold_size = 480 // num_folds\ngap = 5\n\nmodels = []\nmodels_cbt = []\nscores = []\n\nmodel_save_path = 'modelitos_para_despues' \nif not os.path.exists(model_save_path):\n    os.makedirs(model_save_path)\n\ndate_ids = df_train['date_id'].values\n\nfor i in range(num_folds):\n    start = i * fold_size\n    end = start + fold_size\n    if i < num_folds - 1:  \n        purged_start = end - 2\n        purged_end = end + gap + 2\n        train_indices = (date_ids >= start) & (date_ids < purged_start) | (date_ids > purged_end)\n    else:\n        train_indices = (date_ids >= start) & (date_ids < end)\n\n    test_indices = (date_ids >= end) & (date_ids < end + fold_size)\n\n    gc.collect()\n\n    df_fold_train = df_train_feats[train_indices]\n    df_fold_train_target = df_train['target'][train_indices]\n    df_fold_valid = df_train_feats[test_indices]\n    df_fold_valid_target = df_train['target'][test_indices]\n\n    print(f\"Fold {i+1} Model Training\")\n\n    lgb_model = lgb.LGBMRegressor(**lgb_params)\n    lgb_model.fit(\n        df_fold_train[feature_columns],\n        df_fold_train_target,\n        eval_set=[(df_fold_valid[feature_columns], df_fold_valid_target)],\n        callbacks=[\n            lgb.callback.early_stopping(stopping_rounds=100),\n            lgb.callback.log_evaluation(period=100),\n        ],\n    )\n\n    models.append(lgb_model)\n    model_filename = os.path.join(model_save_path, f'doblez_{i+1}.txt')\n    lgb_model.booster_.save_model(model_filename)\n    print(f\"Model for fold {i+1} saved to {model_filename}\")\n\n    fold_predictions = lgb_model.predict(df_fold_valid[feature_columns])\n    fold_score = mean_absolute_error(fold_predictions, df_fold_valid_target)\n    scores.append(fold_score)\n    print(f\":LGB Fold {i+1} MAE: {fold_score}\")\n\n    del df_fold_train, df_fold_train_target, df_fold_valid, df_fold_valid_target\n    gc.collect()\n\naverage_best_iteration = int(np.mean([model.best_iteration_ for model in models]))\n\nfinal_model_params = lgb_params.copy()\n\nnum_model = 1\n\nfor i in range(num_model):\n    final_model = lgb.LGBMRegressor(**final_model_params)\n    final_model.fit(\n        df_train_feats[feature_columns],\n        df_train['target'],\n        callbacks=[\n            lgb.callback.log_evaluation(period=100),\n        ],\n    )\n    models.append(final_model)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Submission**","metadata":{}},{"cell_type":"code","source":"def zero_sum(prices, volumes):\n    std_error = np.sqrt(volumes)\n    step = np.sum(prices) / np.sum(std_error)\n    out = prices - std_error * step\n    return out\n\nif is_infer:\n    import optiver2023\n    env = optiver2023.make_env()\n    iter_test = env.iter_test()\n    counter = 0\n    y_min, y_max = -64, 64\n    qps, predictions = [], []\n    cache = pd.DataFrame()\n\n    lgb_model_weights = weighted_average(models)\n    \n    for (test, revealed_targets, sample_prediction) in iter_test:\n        now_time = time.time()\n        cache = pd.concat([cache, test], ignore_index=True, axis=0)\n        if counter > 0:\n            cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n        feat = generate_all_features(cache)[-len(test):]\n        print(f\"Feat Shape is: {feat.shape}\")\n        \n        lgb_predictions = np.zeros(len(test))\n        for model, weight in zip(models, lgb_model_weights):\n            lgb_predictions += weight * model.predict(feat[feature_columns])\n\n        predictions = lgb_predictions\n        \n        final_predictions = predictions - np.mean(predictions)\n        clipped_predictions = np.clip(final_predictions, y_min, y_max)\n        sample_prediction['target'] = clipped_predictions\n        env.predict(sample_prediction)\n        counter += 1\n        qps.append(time.time() - now_time)\n        if counter % 10 == 0:\n            print(counter, 'qps:', np.mean(qps))\n\n    time_cost = 1.146 * np.mean(qps)\n    print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_prediction.hist(column='target', bins=100, range=[-10,10])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_prediction.to_csv('preds.csv')\nsample_prediction","metadata":{},"execution_count":null,"outputs":[]}]}